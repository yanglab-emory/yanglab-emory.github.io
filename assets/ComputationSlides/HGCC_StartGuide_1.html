<!DOCTYPE html>
<html>

<head>

<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
<title>HGCC_StartGuide_1</title>


<style type="text/css">
body {
  font-family: Helvetica, arial, sans-serif;
  font-size: 14px;
  line-height: 1.6;
  padding-top: 10px;
  padding-bottom: 10px;
  background-color: white;
  padding: 30px; }

body > *:first-child {
  margin-top: 0 !important; }
body > *:last-child {
  margin-bottom: 0 !important; }

a {
  color: #4183C4; }
a.absent {
  color: #cc0000; }
a.anchor {
  display: block;
  padding-left: 30px;
  margin-left: -30px;
  cursor: pointer;
  position: absolute;
  top: 0;
  left: 0;
  bottom: 0; }

h1, h2, h3, h4, h5, h6 {
  margin: 20px 0 10px;
  padding: 0;
  font-weight: bold;
  -webkit-font-smoothing: antialiased;
  cursor: text;
  position: relative; }

h1:hover a.anchor, h2:hover a.anchor, h3:hover a.anchor, h4:hover a.anchor, h5:hover a.anchor, h6:hover a.anchor {
  background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABAAAAAQCAYAAAAf8/9hAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAA09pVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMy1jMDExIDY2LjE0NTY2MSwgMjAxMi8wMi8wNi0xNDo1NjoyNyAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNiAoMTMuMCAyMDEyMDMwNS5tLjQxNSAyMDEyLzAzLzA1OjIxOjAwOjAwKSAgKE1hY2ludG9zaCkiIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OUM2NjlDQjI4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OUM2NjlDQjM4ODBGMTFFMTg1ODlEODNERDJBRjUwQTQiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo5QzY2OUNCMDg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo5QzY2OUNCMTg4MEYxMUUxODU4OUQ4M0REMkFGNTBBNCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PsQhXeAAAABfSURBVHjaYvz//z8DJYCRUgMYQAbAMBQIAvEqkBQWXI6sHqwHiwG70TTBxGaiWwjCTGgOUgJiF1J8wMRAIUA34B4Q76HUBelAfJYSA0CuMIEaRP8wGIkGMA54bgQIMACAmkXJi0hKJQAAAABJRU5ErkJggg==) no-repeat 10px center;
  text-decoration: none; }

h1 tt, h1 code {
  font-size: inherit; }

h2 tt, h2 code {
  font-size: inherit; }

h3 tt, h3 code {
  font-size: inherit; }

h4 tt, h4 code {
  font-size: inherit; }

h5 tt, h5 code {
  font-size: inherit; }

h6 tt, h6 code {
  font-size: inherit; }

h1 {
  font-size: 28px;
  color: black; }

h2 {
  font-size: 24px;
  border-bottom: 1px solid #cccccc;
  color: black; }

h3 {
  font-size: 18px; }

h4 {
  font-size: 16px; }

h5 {
  font-size: 14px; }

h6 {
  color: #777777;
  font-size: 14px; }

p, blockquote, ul, ol, dl, li, table, pre {
  margin: 15px 0; }

hr {
  background: transparent url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAAYAAAAECAYAAACtBE5DAAAAGXRFWHRTb2Z0d2FyZQBBZG9iZSBJbWFnZVJlYWR5ccllPAAAAyJpVFh0WE1MOmNvbS5hZG9iZS54bXAAAAAAADw/eHBhY2tldCBiZWdpbj0i77u/IiBpZD0iVzVNME1wQ2VoaUh6cmVTek5UY3prYzlkIj8+IDx4OnhtcG1ldGEgeG1sbnM6eD0iYWRvYmU6bnM6bWV0YS8iIHg6eG1wdGs9IkFkb2JlIFhNUCBDb3JlIDUuMC1jMDYwIDYxLjEzNDc3NywgMjAxMC8wMi8xMi0xNzozMjowMCAgICAgICAgIj4gPHJkZjpSREYgeG1sbnM6cmRmPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5LzAyLzIyLXJkZi1zeW50YXgtbnMjIj4gPHJkZjpEZXNjcmlwdGlvbiByZGY6YWJvdXQ9IiIgeG1sbnM6eG1wPSJodHRwOi8vbnMuYWRvYmUuY29tL3hhcC8xLjAvIiB4bWxuczp4bXBNTT0iaHR0cDovL25zLmFkb2JlLmNvbS94YXAvMS4wL21tLyIgeG1sbnM6c3RSZWY9Imh0dHA6Ly9ucy5hZG9iZS5jb20veGFwLzEuMC9zVHlwZS9SZXNvdXJjZVJlZiMiIHhtcDpDcmVhdG9yVG9vbD0iQWRvYmUgUGhvdG9zaG9wIENTNSBNYWNpbnRvc2giIHhtcE1NOkluc3RhbmNlSUQ9InhtcC5paWQ6OENDRjNBN0E2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiIHhtcE1NOkRvY3VtZW50SUQ9InhtcC5kaWQ6OENDRjNBN0I2NTZBMTFFMEI3QjRBODM4NzJDMjlGNDgiPiA8eG1wTU06RGVyaXZlZEZyb20gc3RSZWY6aW5zdGFuY2VJRD0ieG1wLmlpZDo4Q0NGM0E3ODY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIgc3RSZWY6ZG9jdW1lbnRJRD0ieG1wLmRpZDo4Q0NGM0E3OTY1NkExMUUwQjdCNEE4Mzg3MkMyOUY0OCIvPiA8L3JkZjpEZXNjcmlwdGlvbj4gPC9yZGY6UkRGPiA8L3g6eG1wbWV0YT4gPD94cGFja2V0IGVuZD0iciI/PqqezsUAAAAfSURBVHjaYmRABcYwBiM2QSA4y4hNEKYDQxAEAAIMAHNGAzhkPOlYAAAAAElFTkSuQmCC) repeat-x 0 0;
  border: 0 none;
  color: #cccccc;
  height: 4px;
  padding: 0;
}

body > h2:first-child {
  margin-top: 0;
  padding-top: 0; }
body > h1:first-child {
  margin-top: 0;
  padding-top: 0; }
  body > h1:first-child + h2 {
    margin-top: 0;
    padding-top: 0; }
body > h3:first-child, body > h4:first-child, body > h5:first-child, body > h6:first-child {
  margin-top: 0;
  padding-top: 0; }

a:first-child h1, a:first-child h2, a:first-child h3, a:first-child h4, a:first-child h5, a:first-child h6 {
  margin-top: 0;
  padding-top: 0; }

h1 p, h2 p, h3 p, h4 p, h5 p, h6 p {
  margin-top: 0; }

li p.first {
  display: inline-block; }
li {
  margin: 0; }
ul, ol {
  padding-left: 30px; }

ul :first-child, ol :first-child {
  margin-top: 0; }

dl {
  padding: 0; }
  dl dt {
    font-size: 14px;
    font-weight: bold;
    font-style: italic;
    padding: 0;
    margin: 15px 0 5px; }
    dl dt:first-child {
      padding: 0; }
    dl dt > :first-child {
      margin-top: 0; }
    dl dt > :last-child {
      margin-bottom: 0; }
  dl dd {
    margin: 0 0 15px;
    padding: 0 15px; }
    dl dd > :first-child {
      margin-top: 0; }
    dl dd > :last-child {
      margin-bottom: 0; }

blockquote {
  border-left: 4px solid #dddddd;
  padding: 0 15px;
  color: #777777; }
  blockquote > :first-child {
    margin-top: 0; }
  blockquote > :last-child {
    margin-bottom: 0; }

table {
  padding: 0;border-collapse: collapse; }
  table tr {
    border-top: 1px solid #cccccc;
    background-color: white;
    margin: 0;
    padding: 0; }
    table tr:nth-child(2n) {
      background-color: #f8f8f8; }
    table tr th {
      font-weight: bold;
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr td {
      border: 1px solid #cccccc;
      margin: 0;
      padding: 6px 13px; }
    table tr th :first-child, table tr td :first-child {
      margin-top: 0; }
    table tr th :last-child, table tr td :last-child {
      margin-bottom: 0; }

img {
  max-width: 100%; }

span.frame {
  display: block;
  overflow: hidden; }
  span.frame > span {
    border: 1px solid #dddddd;
    display: block;
    float: left;
    overflow: hidden;
    margin: 13px 0 0;
    padding: 7px;
    width: auto; }
  span.frame span img {
    display: block;
    float: left; }
  span.frame span span {
    clear: both;
    color: #333333;
    display: block;
    padding: 5px 0 0; }
span.align-center {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-center > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: center; }
  span.align-center span img {
    margin: 0 auto;
    text-align: center; }
span.align-right {
  display: block;
  overflow: hidden;
  clear: both; }
  span.align-right > span {
    display: block;
    overflow: hidden;
    margin: 13px 0 0;
    text-align: right; }
  span.align-right span img {
    margin: 0;
    text-align: right; }
span.float-left {
  display: block;
  margin-right: 13px;
  overflow: hidden;
  float: left; }
  span.float-left span {
    margin: 13px 0 0; }
span.float-right {
  display: block;
  margin-left: 13px;
  overflow: hidden;
  float: right; }
  span.float-right > span {
    display: block;
    overflow: hidden;
    margin: 13px auto 0;
    text-align: right; }

code, tt {
  margin: 0 2px;
  padding: 0 5px;
  white-space: nowrap;
  border: 1px solid #eaeaea;
  background-color: #f8f8f8;
  border-radius: 3px; }

pre code {
  margin: 0;
  padding: 0;
  white-space: pre;
  border: none;
  background: transparent; }

.highlight pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }

pre {
  background-color: #f8f8f8;
  border: 1px solid #cccccc;
  font-size: 13px;
  line-height: 19px;
  overflow: auto;
  padding: 6px 10px;
  border-radius: 3px; }
  pre code, pre tt {
    background-color: transparent;
    border: none; }

sup {
    font-size: 0.83em;
    vertical-align: super;
    line-height: 0;
}

kbd {
  display: inline-block;
  padding: 3px 5px;
  font-size: 11px;
  line-height: 10px;
  color: #555;
  vertical-align: middle;
  background-color: #fcfcfc;
  border: solid 1px #ccc;
  border-bottom-color: #bbb;
  border-radius: 3px;
  box-shadow: inset 0 -1px 0 #bbb
}

* {
	-webkit-print-color-adjust: exact;
}
@media screen and (min-width: 914px) {
    body {
        width: 854px;
        margin:0 auto;
    }
}
@media print {
	table, pre {
		page-break-inside: avoid;
	}
	pre {
		word-wrap: break-word;
	}
  body {
    padding: 2cm; 
  }
}
</style>


</head>

<body>

<h1 id="toc_0">Introduction to New Human Genetics Compute Cluster (HGCC)</h1>

<h2 id="toc_1">HGCC Cluster Structure</h2>

<ul>
<li><a href="https://hgcc.emory.edu"><strong>New HGCC Website</strong></a> : Access within Emory network or after login to Emory VPN.</li>
<li><strong>Operating System:</strong> Red Hat Enterprise Linux 8<br></li>
<li><strong>Shared Storage (beegfs)</strong> : 1.3 PB (10TB per lab)</li>
<li><strong>1 Head/Gateway/Login Node <code>head.hgcc</code></strong>; <strong>1 Data Transfer Node (DTN)</strong>

<ul>
<li>No CPU or memory intensive programs are allowed on the headnode and the DTN. </li>
<li>2CPU and 8G RAM per user</li>
<li>Default interactive job time: 1 hour</li>
</ul></li>
<li><strong>14 Compute Nodes of the same Partition</strong>:

<ul>
<li>64 cores per node; 16GB RAM per core</li>
<li>High-bandwidth: 100Gb/s interconnect fabric. Compute node and storage volumes.<br></li>
<li>10Gb/s uplinks to the Emory fiber-optic backbone. Between Emory clusters.</li>
<li>Scheduling system: <a href="https://slurm.schedmd.com/documentation.html"><strong>Slurm</strong></a></li>
</ul></li>
<li><strong>Shared Fast Scratch Storage</strong>: <code>/scratch/</code>

<ul>
<li>84 TB, first come first serve</li>
<li>All files will be deleted after 30 days </li>
<li>Recomend to use for jobs that need frequent large data file access, I-O intensive jobs.</li>
</ul></li>
<li><strong>Data Drive of YangLab</strong>: <code>/nfs/yangfss2/data/</code>

<ul>
<li>This is our previous data drive 2 (350TB) in the old cluster. Our data drive 1 (45TB) is retired.</li>
<li>A softlink of the datadrive has been created under your home directory: <code>~/yangfss2/</code></li>
<li><code>commons/[user]</code> : User work directories. 

<ul>
<li>All data files under your user work directories on our lab data drive 1 and 2 on the old HGCC cluster are copied here. </li>
<li>All files on the old hgcc main disk <code>icebreaker</code> are not accessible and not coped into this new system.</li>
</ul></li>
<li><code>shared/**</code> : Shared data directories and software directory <code>shared/Software/</code> for the lab.</li>
<li><code>projects/</code> : Project directories shared by the lab.</li>
<li><code>public/</code> : Files to be shared with other groups. Public to be viewed by all users on the cluster.</li>
</ul></li>
<li><strong>All files are not backed up on the cluster.</strong></li>
</ul>

<p><img src="./hgcc_structure.png" alt="hgcc_architecture"></p>

<h2 id="toc_2">Login to HGCC</h2>

<ol>
<li><p>Login to <a href="https://it.emory.edu/security/vpn.html"><strong>Emory VPN</strong></a> . </p>

<ul>
<li><a href="https://it.emory.edu/security/protecting-data/virus_protection.html"><strong>Anti-virus Software</strong></a> is needed for your local computer. </li>
<li><a href="https://it.emory.edu/vpntools/index.html"><strong>BIG-IP Edge Client</strong></a> is needed for VPN connection.</li>
<li>Contact <a href="https://emory.service-now.com/sp?sysparm_stack=no"><strong>Emory IT</strong></a> if further help is needed.</li>
</ul></li>
<li><p><strong>Login using <code>ssh</code></strong> by typing the following command in the terminal with your Emory Net ID and Emory password:
<code>ssh &lt;Emory_Net_ID&gt;@hgcc.emory.edu</code></p>

<ul>
<li>Terminal App is available on Mac OS X and Linux system</li>
<li>Under Windows 10, open up a Powershell (search for “Powershell” in the search tool next to the Start menu) and type ssh. (Older versions of the operating system may require the installation of a third party ssh client, like <a href="https://www.putty.org/">Putty</a>.)</li>
</ul></li>
<li><p><a href="http://www.linuxproblem.org/art_9.html"><strong>SSH login without typing password</strong></a> : repeat this setup for each of your local computer.</p>

<ul>
<li><p>First, generate a pair of authentication key under the home directory on your local computer: <code>ssh-keygen -t rsa</code>. </p>

<ul>
<li> Your identification has been saved in <code>~/.ssh/id_rsa</code>.</li>
<li> Your public key has been saved in <code>~/.ssh/id_rsa.pub</code>. </li>
</ul></li>
<li><p>Second, run the following command under the home directory on your local computer to create a directory <code>~/.ssh</code> under your home directory on the HGCC cluster and append your local authentication key to <code>~/.ssh/authorized_keys</code> on HGCC:</p>

<ul>
<li>  <code>ssh-copy-id &lt;Emory_Net_ID&gt;@hgcc.emory.edu</code></li>
</ul></li>
</ul></li>
</ol>

<h2 id="toc_3">Mount Remote Cluster Home Directory to Local Computer</h2>

<h3 id="toc_4">Use <a href="https://winscp.net/eng/index.php">WinSCP</a> for Windows system</h3>

<h3 id="toc_5">Use <a href="https://github.com/libfuse/sshfs">sshfs</a> for MAC</h3>

<ol>
<li>Install the latest <a href="https://github.com/libfuse/sshfs/releases">SSHFS</a>.</li>
<li>Install the lastest <a href="https://osxfuse.github.io/">macFUSE</a>.</li>
<li>Mount the remote HGCC home directory <code>/home/&lt;jyang51&gt;/</code> to  local directory <code>~/HGCC/</code>. Replace <code>&lt;jyang51&gt;</code> by your Emory<em>Net</em>ID.
<code>sshfs &lt;jyang51&gt;@hgcc.emory.edu:/home/&lt;jyang51&gt;/ ~/HGCC/ -o auto_cache -ovolname=HGCC -o follow_symlinks</code></li>
<li>Now you can access all files on HGCC through the mounted local directory.</li>
</ol>

<h2 id="toc_6">Transfer Data Files to/from Cluster</h2>

<ol>
<li><p><strong>Command <code>rsync</code> is recommended</strong></p>

<ul>
<li>Transfer data from local machine to cluster, from cluster to local machine</li>
<li>Transfer data within cluster or across clusters </li>
<li>See <a href="https://phoenixnap.com/kb/rsync-command-linux-examples">Example usage</a></li>
</ul>

<p><code>rsync [options] [SOURCE] [DESTINATION]</code></p></li>
<li><p><strong>Command <code>scp</code> can also do the job</strong></p>

<ul>
<li>See <a href="https://www.hypexr.org/linux_scp_help.php">Example usage</a>
<code>scp &lt;path_to_local_file&gt; &lt;Emory_Net_ID&gt;@hgcc.emory.edu:&lt;destination_remote_path_on_hgcc&gt;</code></li>
</ul></li>
</ol>

<h2 id="toc_7">Useful Basic Linux Commands for Handling Data Files on Cluster</h2>

<ol>
<li>Command <code>rsync</code> is recommended for moving data files within the cluster, between local and cluster, between clusters.</li>
<li>Command <code>cp</code> also copys data.</li>
<li>Delete data or directory by <code>rm</code>.</li>
<li>Make directories by <code>mkdir</code>.</li>
<li>Move a directory by <code>mv</code>.</li>
<li>List all files under a directory by <code>ls</code>.</li>
<li>List all files with their sizes by <code>ls -l -h</code>.</li>
<li>Use <code>vi</code> or <code>nano</code> to edit text files on the cluster. Recomend edit text files through local mounted directory.<br></li>
<li>Read text file on the cluster by <code>less</code>, <code>cat</code>. <code>less -S</code> is recommended for viewing large text files on cluster.</li>
<li>Consider gzip your text file to save space by <code>gzip</code>.</li>
<li>Unzip gzipped text file by <code>gunzip [file_name.gz]</code>.</li>
<li>Use <code>tar</code> for zipping and unzipping directories.</li>
<li>Command <code>man</code> to see user manual/instructions of Linux tools, e.g., <code>man ls</code> .</li>
<li>Use pipe <code>|</code> to take output from the command before <code>|</code> as the input for the command after <code>|</code>. For example, <code>cat xx_file.txt | head</code> will print the top 10 rows of the <code>xx_file.txt</code> file on the bash window.</li>
<li>Create alias by <code>alias</code> to set up your own shortcup of commands in the <code>~/.bashrc</code> file. See <a href="https://phoenixnap.com/kb/linux-alias-command">Examples</a>. Example alias commands such as <code>alias c=&#39;clear&#39;</code>, can be seen through <code>/home/jyang51/.bashrc</code>. This would set up <code>c</code> as an alias/short-cut for the command <code>clear</code>.</li>
</ol>

<p><strong>Command <code>man [command]</code> would give help page for Linux commands, e.g., <code>man rsync</code>. Type <code>q</code> to exit from the help page.</strong></p>

<h2 id="toc_8">Strategies for Using HGCC</h2>

<ol>
<li><strong>Only use headnode to download/move data and submit jobs. Never run large computation jobs on headnode.</strong></li>
<li><strong>Login to an interactive compute node by command <code>srun -N 1 -n 1 --pty bash</code>.</strong>

<ul>
<li><code>-N</code> is the number of nodes, <code>-n</code> is number of tasks, <code>--pty</code> gives you a pseudo terminal that runs bash. </li>
<li>Use <strong>Interactive bash session</strong> for testing your jobs.</li>
<li>Multiple interactive sessions could be opened.</li>
</ul></li>
<li><strong>Wrap your jobs in shell scripts and submit jobs to compute node by <code>sbatch</code> under headnode.</strong>

<ul>
<li><a href="https://slurm.schedmd.com/job_array.html"><strong>Job Array</strong></a> is recommended for a large number of jobs of the same function.</li>
<li>Think about breaking your one big job into multiple small jobs/steps and submit array jobs. Multiple small jobs may be more efficient than a single large job. </li>
<li>Default running time for a job is up to <strong>30 days</strong></li>
</ul></li>
<li><strong>Use scratch space (</strong>84TB<strong>) <code>/scratch/</code> to avoid extensive I/O between compute node memory and storage disks.</strong> 

<ol>
<li>Create temperary working directory under the scratch directory, <code>mkdir -p /scratch/tmp_jyang51/</code>, with a unique directory name <code>tmp_jyang51</code>. </li>
<li>Copy data into temperary working directory in the scratch space, <code>rsync [path_to_data_file]/[data_file_name] /scratch/tmp_jyang51/</code>. </li>
<li>Write output files into the temperary working directory, <code>/scratch/tmp_jyang51/</code>. </li>
<li>Copy output files back to your storage, <code>rsync /scratch/tmp_jyang51/ [path_to_output_files]/</code>.</li>
<li>Delete temperary working directory, <code>rm -rf /scratch/tmp_jyang51</code>.</li>
<li>All files in the scratch space will be deleted after 30 days.</li>
</ol></li>
<li><strong>Do not make another copy of the same data unless you need to make changes.</strong></li>
<li><strong>Delete files that you will no longer to use.</strong></li>
<li><strong>Keep your working directoreis organized and cleaned.</strong></li>
<li><strong>Write <code>README.md</code> file for each data directory.</strong></li>
</ol>

<h2 id="toc_9">Using a Software on HGCC</h2>

<h3 id="toc_10">Check if a software is available on the cluster</h3>

<ol>
<li>Use command <code>spack find -x</code> to list all installed software modules on HGCC.</li>
<li>Use command <code>spack find --loaded</code> to list all loaded software modules.</li>
<li>Check if an executible file of an installed or loaded software is available under <code>${PATH}</code> by <code>which [software_name]</code>.</li>
<li>Add a symbolic link of the executible file under <code>${PATH}</code> to avoid typing full directory of the executible file. </li>
</ol>

<div><pre><code class="language-none">export PATH=&quot;~/.local/bin:$PATH&quot; # add local directory to PATH. add this to ~/.bashrc file to avoid running this for each session.
echo $PATH; # show current PATH directories
cd ~/.local/bin; #
ln -s [directory_to_executible_file]; # create symbolic link to the software</code></pre></div>

<h3 id="toc_11">Using a software installed on the cluster by <code>spack</code></h3>

<ol>
<li>Load a software module into the current session by <code>spack load [software_name]</code>.

<ul>
<li>For example, use command <code>spack load plink2@2.00a4.3</code> to load <strong>plink</strong>.</li>
<li>Type command <code>plink2 --help</code> to see plink user mannual after loading <strong>plink</strong>.</li>
<li>Unload <strong>plink</strong> module by <code>spack unload plink2@2.00a4.3</code>.</li>
</ul></li>
<li>Type command <code>R</code> in the current session to start an R session after loading <strong>R</strong> by <code>spack load r@4.4.0</code>.<br></li>
<li>Type command <code>python</code>  in the session to start a python session after loading <strong>Anaconda3</strong> by <code>spack load anaconda3@2023.09-0</code>.</li>
<li>One can open text scripts on HGCC by local text/script Editor (e.g., <strong><a href="https://www.sublimetext.com/">Sublime</a></strong>, <strong><a href="https://code.visualstudio.com/">Visual Studio Code</a></strong>) after mounting cloud directory to local machine, and just copy/paste command into these <strong>bash</strong>, <strong>R</strong>, or <strong>python</strong> sessions.</li>
</ol>

<h3 id="toc_12">Install a software without root access</h3>

<ol>
<li>Install software under user home directory, e.g., <code>~/.local/</code>. Include <code>~/.local/bin/</code> in the <code>$PATH</code> environment variable. Create a symbolic link to software executible file under <code>~/.local/bin/</code>.</li>
<li>Create a virtual python environment by <code>conda</code> to install python libraries under your virtual python environment. </li>
<li>Submit a ticket through the HGCC website to request a software to be installed by IT.</li>
</ol>

<h3 id="toc_13">Share installed software with the lab</h3>

<ul>
<li>Make a soft link of the executible tool under <code>/nfs/yangfss2/data/shared/bin</code> by the following commands:</li>
</ul>

<div><pre><code class="language-none">export PATH=&quot;/nfs/yangfss2/data/shared/bin:$PATH&quot;; # include this line of command in your `~/.bashrc` file to automatically run this for each session.
cd /nfs/yangfss2/data/shared/bin
ln -s [tool_directory]</code></pre></div>

<h2 id="toc_14">Setup one&#39;s <code>~/.bashrc</code> file</h2>

<ul>
<li>See <a href="https://www.freecodecamp.org/news/bashrc-customization-guide/">useful things</a> that you can setup your own <code>~/.bashrc</code> file under your home directory to automaticlly run commands for each session. 

<ul>
<li>Set up <code>$PATH</code> system variables. See <a href="https://www.baeldung.com/linux/path-variable">example</a>.</li>
<li>Create alias by <code>alias</code> in <code>~/.bashrc</code> to set up your own shortcup of commands. See <a href="https://phoenixnap.com/kb/linux-alias-command">Examples</a>.</li>
<li>Setup environment variables. See <a href="https://www.cyberciti.biz/faq/set-environment-variable-linux/">example</a>.</li>
<li>Load software modules.</li>
</ul></li>
</ul>

<h2 id="toc_15">Submit Jobs by SLURM</h2>

<h3 id="toc_16">Basic SLURM commands</h3>

<ul>
<li><strong>Use command <code>sbatch</code> to submit jobs</strong>. Set arguments to <code>sbatch</code> in a wrapper shell (job submission) script, for example, you may use command <code>sbatch norm_sim.sh</code> to submit an array job to run 10 times of simulations, with <code>norm_sim.sh</code> given as follows: </li>
</ul>

<div><pre><code class="language-none">#!/bin/bash
#SBATCH --job-name=normal_sim ## specify job name
#SBATCH --nodes=1 ## request 1 node 
#SBATCH --mem=8G ## request 8G memory
#SBATCH --cpus-per-task=4 ## request 4 cpus/cores per job
#SBATCH --time=24:00:00 ## specify job running time for 24 hrs
#SBATCH --output=./SLURM_OUT/%x_%A_%a.out ## specify slurm output file directory
#SBATCH --error=./SLURM_OUT/%x_%A_%a.err ## specify slurm error file directory
#SBATCH --array=0-10 ## Submitting 10 instances of commands listed below


## The following commands will be run for 10 times by 10 jobs under the specified array. Each with their unique task ID given by $SLURM_ARRAY_TASK_ID, in {1..10}.

## Change working directory
cd /home/jyang51/yangfss2/public/ExampleData

## Create SLURM_OUT/ under the current working directory to save slurm output and error files
mkdir -p ./SLURM_OUT/


## Print SLURM array task ID $SLURM_ARRAY_TASK_ID (1..10)
echo &quot;My SLURM_ARRAY_TASK_ID: &quot; $SLURM_ARRAY_TASK_ID


## Load R software
spack load r@4.4.0


## Using the Rscript to simulate a vector x from standard normal distribution and write x to a text data file under /home/jyang51/yangfss2/public/ExampleData/
## Use the SLURM array task ID to save unique output data files, or configue the job.
Rscript /home/jyang51/yangfss2/public/ExampleScripts/norm_sim.R /home/jyang51/yangfss2/public/ExampleData/ $SLURM_ARRAY_TASK_ID</code></pre></div>

<ul>
<li>And example <code>norm_sim.R</code> script given as follows:</li>
</ul>

<div><pre><code class="language-none">#!/usr/bin/env Rscript
Sys.setlocale(&#39;LC_ALL&#39;, &#39;C&#39;)

###############
options(stringsAsFactors=F)


###############
args=(commandArgs(TRUE))
print(args)
if(length(args)==0) {
  stop(&quot;Error: No arguments supplied!&quot;)
} else {
  out_prefix = args[[1]]
  n_sim = args[[2]]
}

x = rnorm(100, mean = 0, sd = 1)

print(paste(&quot;mean of simulated data =&quot;, mean(x)))

print(paste(&quot;standard deviation of simulated data =&quot;, sd(x)))

print(&quot;Write simulated data to a text file:&quot;)

write.table(data.frame(x = x), file = paste0(out_prefix, &quot;sim_&quot;, n_sim, &quot;.txt&quot;), quote = FALSE, sep = &quot;\t&quot;, row.names = FALSE)</code></pre></div>

<ul>
<li><strong>Use command <code>squeue</code> to display current slurm job queue</strong>.</li>
<li><strong>Use command <code>scancel [jobid]</code> used to cancel/kill a job</strong>.</li>
<li><strong>Use command <code>scontrol</code> used to show information about running or pending jobs</strong>.

<ul>
<li><code>scontrol show job [jobid]</code> to show system details of a submitted job.</li>
</ul></li>
<li><p><strong>Use command <code>srun</code> to run an interactive instance: <code>srun --pty bash</code>.</strong></p>

<ul>
<li>Open an interactive session to test your <em>R</em> code: </li>
</ul>

<div><pre><code class="language-none">srun -N 1 -n 1 --pty bash
spack load r@4.4.0
R</code></pre></div>

<ul>
<li>Simply exit the interactive session by <code>exit</code>.</li>
</ul></li>
<li><p><strong>Use command <code>sinfo</code> to report the state of the cluster partition and nodes</strong>.   </p></li>
</ul>

<h2 id="toc_17">Monitoring storage usage</h2>

<ol>
<li>Check storage space for all data drives on HGCC by <code>df -h</code></li>
<li>Check space used by current working directory by <code>du -h -d1</code></li>
</ol>

<h2 id="toc_18">Additional Resources</h2>

<ul>
<li><a href="https://hgcc.emory.edu/#help">New HGCC Website</a>. Need to be in Emory network or Emory VPN to access.</li>
<li><a href="https://spack.io/">Spack website</a>.</li>
<li><a href="https://linuxcommand.org/tlcl.php">The Linux Command Line</a> book.</li>
</ul>




</body>

</html>
